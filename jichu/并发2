1.# 进程池
    # 一般进程池中的数量是cpu个数+1
    # ret = map(func,iterable)
        # 异步 自带close和join
        # 所有结果的[]
    # apply
        # 同步的:只有当func执行完之后,才会继续向下执行其他代码
        # ret = apply(func,args=())
        # 返回值就是func的return
    # apply_async
        # 异步的:当func被注册进入一个进程之后,程序就继续向下执行
        # apply_async(func,args=())
        # 返回值 : apply_async返回的对象obj
        #          为了用户能从中获取func的返回值obj.get()
        # get会阻塞直到对应的func执行完毕拿到结果
        # 使用apply_async给进程池分配任务,
        # 需要先close后join来保持多进程和主进程代码的同步性,即先执行完进程池中的任务后执行主进程
        
2.回调函数
  爬虫的例子demo1:(爬取数据的长度)
  import requests
  from urllib.request import urlopen
  from multiprocessing import Pool
  # 200 网页正常的返回
  # 404 网页找不到
  # 502 504
  def get(url):
      response = requests.get(url)
      if response.status_code == 200:
          return url,response.content.decode('utf-8')

  def get_urllib(url):
      ret = urlopen(url)
      return ret.read().decode('utf-8')

  def call_back(args):
      url,content = args
      print(url,len(content))

  if __name__ == '__main__':
      url_lst = [
          'https://www.cnblogs.com/',
          'http://www.baidu.com',
          'https://www.sogou.com/',
          'http://www.sohu.com/',
      ]
      p = Pool(5)
      for url in url_lst:
          p.apply_async(get,args=(url,),callback=call_back)
      p.close()
      p.join()
  爬虫的例子demo2:(爬取数据的长度)
  import re
  from urllib.request import urlopen
  from multiprocessing import Pool
  def get_page(url,pattern):
      response=urlopen(url).read().decode('utf-8')
      return pattern,response   # 正则表达式编译结果 网页内容

  def parse_page(info):
      pattern,page_content=info
      res=re.findall(pattern,page_content)
      #print(res) [(第一项),(第二项)]
      for item in res:
          dic={
              'index':item[0].strip(),
              'title':item[1].strip(),
              'actor':item[2].strip(),
              'time':item[3].strip(),
          }
          print(dic)
  if __name__ == '__main__':
      regex = r'<dd>.*?<.*?class="board-index.*?>(\d+)</i>.*?title="(.*?)".*?class="movie-item-info".*?<p class="star">(.*?)</p>.*?<p class="releasetime">(.*?)</p>'
      pattern1=re.compile(regex,re.S)
      url_dic={'http://maoyan.com/board/7':pattern1}
      p=Pool()
      res_l=[]
      for url,pattern in url_dic.items():
          res=p.apply_async(get_page,args=(url,pattern),callback=parse_page)
          res_l.append(res)
      for i in res_l:
          i.get()
          
  3.线程：
  import os
  import time
  #定义线程的两种方式
  #第一种
  # import time
  # from threading import Thread
  # def func(n):
  #     time.sleep(1)
  #     print(n)
  #
  # for i in range(10):
  #     t=Thread(target=func,args=(i,)) #0~9 一起被打印出来 实现了多线程
  #     t.start()
  #第二种
  # import time
  # from threading import Thread
  # class MyThread(Thread):
  #     def __init__(self,arg):
  #         super().__init__()
  #         self.arg=arg
  #     def run(self):
  #         time.sleep(1)
  #         print(self.arg)
  # for i in range(10):
  #     t=MyThread(i)
  #     t.start() #0~9 一起被打印出来 实现了多线程

  # 对于同一个进程的多个线程，他们的数据是共享的
  import time
  # from threading import Thread
  # def func(a,b):
  #     global g
  #     g = 0
  #     print(g,os.getpid()) #0
  # 
  # g = 100
  # t_lst = []
  # for i in range(10):
  #     t = Thread(target=func,args=(i,5))
  #     t.start()
  #     t_lst.append(t)
  # for t in  t_lst : t.join()
  # print(g) #0


  # 进程 是 最小的 资源分配单位（内存）
  #进程中存放的有导入的模块，文件存放的位置，内置的函数，代码
  #每个线程有自己的栈
  # 线程 是 操作系统调度的最小单位
  # 线程直接被CPU执行,进程内至少含有一个线程,也可以开启多个线程
      # 开启一个线程所需要的时间要远远小于开启一个进程
      # 多个线程内部有自己的数据栈,数据不共享
      # 全局变量在多个线程之间是共享的
  #多个线程同时对数据进行操作会造成数据的不安全，因此需要加锁GIL锁(即全局解释器锁)。
  # GIL锁(即全局解释器锁)--锁的是线程，线程只有拿到钥匙以后才能去操作数据，同一个时刻只有有一个线程访问CPU，不能实现多线程
  # 首先需要明确的一点是GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。
  # Python也一样，同样一段代码可以通过CPython，PyPy，Psyco等不同的Python执行环境来执行。像其中的JPython就没有GIL。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。
  # 所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL。
  # 简单来说，在Cpython解释器中，因为有GIL锁的存在同一个进程下开启的多线程，同一时刻只能有一个线程执行，无法利用多核优势。
  #多线程中，python虚拟机的执行方式：
  # a.设置GIL（全局解释器锁）相当于拿钥匙
  # b.切换到一个线程去执行（使用CPU执行）
  # c.运行指定数量的字节码指令或者线程主动让出执行权
  # d.把线程设置为睡眠状态
  # e。解锁GIL(还锁)
  # d.重复以上步骤
  #所以GIL锁的是线程，同一时刻只能有一个线程执行
  # 高CPU : 计算类 --- 高CPU利用率
  # 高IO  : 爬取网页 200个网页
          # qq聊天   send recv
          # 处理日志文件 读文件
          # 处理web请求
          # 读数据库 写数据库
  #多进程中不能有input操作，但是多线程中可以有input操作
  # import time
  # from threading import Thread
  # from multiprocessing import Process
  # def func(n):
  #     n + 1
  #
  # if __name__ == '__main__':
  #       线程执行的时间
  #     start = time.time()
  #     t_lst = []
  #     for i in range(100):
  #         t = Thread(target=func,args=(i,))
  #         t.start()
  #         t_lst.append(t)
  #     for t in t_lst:t.join()
  #     t1 = time.time() - start
  #     进程执行的时间
  #     start = time.time()
  #     t_lst = []
  #     for i in range(100):
  #         t = Process(target=func, args=(i,))
  #         t.start()
  #         t_lst.append(t)
  #     for t in t_lst: t.join()
  #     t2 = time.time() - start
  #     print(t1,t2)
  #结论 线程的执行效率更高
  4.线程中的其他方法
  import time
  import threading
  #threading.current_thread() 查看线程名
  #threading.get_ident()查看线程id
  def wahaha(n):
      time.sleep(0.5)
      print(n,threading.current_thread(),threading.get_ident()) #threading.current_thread() 打印子线程名  threading.get_ident()打印子线程id

  for i in  range(10):
      threading.Thread(target=wahaha,args=(i,)).start() #启动十个线程
  print(threading.active_count())    # 11 查看当前所有存活的线程数量 (10子线程+1个主线程)
  print(threading.current_thread())  #打印主线程名 <_MainThread(MainThread, started 59988)>
  print(threading.enumerate()) #返回一个包含正在运行的线程名称的list
  
  5.多线程实现socketserver
  server端：
    #主线程接收连接后，子线程负责处理收发数据
    import socket
    from threading import Thread

    def chat(conn):
        conn.send(b'hello')
        msg = conn.recv(1024).decode('utf-8')
        print(msg)
        conn.close()

    sk=socket.socket()
    sk.bind(('127.0.0.1',8080))
    sk.listen()
    while True:
        conn,addr=sk.accept()
        Thread(target=chat,args=(conn,)).start()
    sk.close()
   client端：
    import socket
    sk = socket.socket()
    sk.connect(('127.0.0.1',8080))
    msg = sk.recv(1024)
    print(msg)
    inp = input('>>> ').encode('utf-8')
    sk.send(inp)
    sk.close()
    
   6.守护线程
   # 守护进程随着主进程代码的执行结束而结束 (守护的是主进程)
    # 守护线程会在主线程结束之后等待其他子线程的结束才结束 （守护的是所有线程）
    #例如以下的demo，线程t会等待线程t2,主线程都执行完毕以后才结束
    import time
    from threading import Thread
    def func1():
        while True:
            print('*'*10)
            time.sleep(1)
    def func2():
        print('in func2')
        time.sleep(5)

    t = Thread(target=func1,)
    t.daemon = True
    t.start()
    t2 = Thread(target=func2,)
    t2.start()
    print('主线程')

    # 主进程在执行完自己的代码之后不会立即结束 而是等待子进程结束之后回收子进程的资源
    #主进程的代码执行结束以后，并不影响非守护进程的子进程的运行
    #例如以下demo,主进程代码结束后还等待子进程的结束
    # import time
    # from multiprocessing import Process
    # def func():
    #     time.sleep(5)
    #
    # if __name__ == '__main__':
    #         Process(target=func).start()
   
   7.线程间的锁：
    #线程间的数据不安全
    #当线程1获取GIL锁，得到CPU的执行权，取数据n进行操作，
    # 但当线程1还没将数据n的结果返回时，cpu的时间片已到，线程2获取了GIL锁，得到CPU的执行权，取数据n进行操作。这时就照成了数据的不安全
    # import time
    # from threading import Lock,Thread
    # def func(lock):
    #     global n
    #     temp = n
    #     time.sleep(0.2)
    #     n = temp - 1
    #
    # n = 10
    # t_lst = []
    # lock = Lock()
    # for i in range(10):
    #     t = Thread(target=func,args=(lock,))
    #     t.start()
    #     t_lst.append(t)
    #
    # for t in  t_lst: t.join()
    # print(n) #结果大于0   9，8，7等都有可能
    #总结：
    #进程的不安全是由于多个进程同时对某个数据进行操作
    #线程的不安全：虽然同一个时刻只有一个线程在执行，但是某个线程还没将数据的操作结果写回就失去了GIL锁，另一个线程便获取了执行权而对数据进行了操作

    # Lock 互斥锁
    # import time
    # from threading import Lock,Thread
    # # Lock 互斥锁
    # def func(lock):
    #     global n
    #     lock.acquire()
    #     temp = n
    #     time.sleep(0.2)
    #     n = temp - 1
    #     lock.release()
    #
    # n = 10
    # t_lst = []
    # lock = Lock()
    # for i in range(10):
    #     t = Thread(target=func,args=(lock,))
    #     t.start()
    #     t_lst.append(t)
    #
    # for t in  t_lst: t.join()
    # print(n) #0

    #以下demo发生死锁
    # 当一个科学家拿了面条，另一个科学家拿了叉子就会死锁
    # 科学家拿了面条和拿了叉子才能吃面
    # noodle_lock  = Lock()
    # fork_lock = Lock()
    # def eat1(name):
    #     noodle_lock.acquire()
    #     print('%s拿到面条啦'%name)
    #     fork_lock.acquire()
    #     print('%s拿到叉子了'%name)
    #     print('%s吃面'%name)
    #     fork_lock.release()
    #     noodle_lock.release()
    #
    # def eat2(name):
    #     fork_lock.acquire()
    #     print('%s拿到叉子了'%name)
    #     time.sleep(1)
    #     noodle_lock.acquire()
    #     print('%s拿到面条啦'%name)
    #     print('%s吃面'%name)
    #     noodle_lock.release()
    #     fork_lock.release()
    #
    # Thread(target=eat1,args=('alex',)).start()
    # Thread(target=eat2,args=('Egon',)).start()
    # Thread(target=eat1,args=('bossjin',)).start()
    # Thread(target=eat2,args=('nezha',)).start()

    #递归锁可以解决死锁问题，相当于同时获取多把锁
    import time
    from threading import Lock,Thread
    from threading import RLock   # 递归锁
    fork_lock = noodle_lock  = RLock()
    # 对于递归锁来说，当线程获取了其中一把锁，其他锁就不能被其他线程获取
    def eat1(name):
        noodle_lock.acquire()            # 一把钥匙
        print('%s拿到面条啦'%name)
        fork_lock.acquire()
        print('%s拿到叉子了'%name)
        print('%s吃面'%name)
        fork_lock.release()
        noodle_lock.release()

    def eat2(name):
        fork_lock.acquire()
        print('%s拿到叉子了'%name)
        time.sleep(1)
        noodle_lock.acquire()
        print('%s拿到面条啦'%name)
        print('%s吃面'%name)
        noodle_lock.release()
        fork_lock.release()

    Thread(target=eat1,args=('alex',)).start()
    Thread(target=eat2,args=('Egon',)).start()
    Thread(target=eat1,args=('bossjin',)).start()
    Thread(target=eat2,args=('nezha',)).start()

8.信号量
import time
from threading import Semaphore,Thread
def func(sem,a,b):
    sem.acquire()
    time.sleep(1)
    print(a+b)
    sem.release()

sem = Semaphore(4)
for i in range(10):
    t = Thread(target=func,args=(sem,i,i+5))
    t.start()  #结果4个4个打印
    
9.事件：
# 事件被创建的时候
# False状态
    # wait() 阻塞
# True状态
    # wait() 非阻塞
# clear 设置状态为False
# set  设置状态为True

#多线程实现数据库连接的demo
#  第一个线程 : 测试网络连通性
        # 等待一个信号 告诉网络之间的连通性
        # 当网络连通时，设置事件的标志为True
#  第二个线程 : 连接数据库
#       当检测网络是连通时，连接数据库。否则就阻塞，等待网络连通的事件标志

import time
import random
from threading import Thread,Event
def check_web(e):
    time.sleep(random.randint(0,3)) #模拟网络延迟
    e.set() #网络连通

def connenct_db(e):
    count=0
    while count<3: #最多连接3次，每次尝试连接数据库的时间不超过1秒
        # e.wait() #一直阻塞，直到网络连通
        e.wait(1) ##一直阻塞，直到网络连通或者阻塞时间到达一秒
        if e.is_set()==True:
            print('连接数据库')
            break
        else:
            print('连接失败')
            count+=1
    else: #即count>=3
        raise TimeoutError('数据库连接超时')

e=Event()
Thread(target=check_web,args=(e,)).start()
Thread(target=connenct_db,args=(e,)).start()

10.condition
# 条件
# acquire release wait notify
# 一个条件被创建时默认有一个False状态，该False状态会影响wait一直处于等待状态
# notify(n)参数必须为int类型，唤醒n个被阻塞的线程
from threading import Thread,Condition
def func(con,i):
        con.acquire()
        con.wait()
        print('in第%s个函数里' %i)
        con.release()
con=Condition()
for i in range(10):
    Thread(target=func,args=(con,i)).start()
while True:
    n=int(input('>>>'))
    con.acquire()
    con.notify(n)#唤醒n个线程
    con.release()
    
11.Timer定时器
# Timer(n,func) 等待n秒后执行函数func
#例如下面的demo实现每5秒钟执行一次时间同步 （通过递归时间）
from threading import Timer
def hello(name):
    print ("时间同步 %s\n" % name)
    global timer
    timer = Timer(2.0, hello, ["Hawk"])
    timer.start()

if __name__ == "__main__":
    t = Timer(2.0, hello, ["Hawk"])
    t.start()
    
12.队列
# queue #线程安全的
import queue
q = queue.Queue()  # 队列 先进先出
# q.put() #若队列已满则阻塞
# q.get() #若队列为空则阻塞
# q.put_nowait()#队列已满也不阻塞，而是过一会来放值（会报错）
# q.get_nowait()#队列已空也不阻塞，而是过一会来取值（会报错）

# q = queue.LifoQueue()  # 栈 先进后出
# q.put(1)
# q.put(2)
# q.put(3)
# print(q.get()) #3
# print(q.get()) #2

q = queue.PriorityQueue()  # 优先级队列
q.put((20,'a')) #put(优先级，元素) 数字越小优先级越高
q.put((10,'b'))
q.put((30,'c'))
q.put((-5,'d'))
q.put((-5,'z'))
q.put((1,'?'))
print(q.get()) #(-5, 'd') -5的优先级最高，优先级相同时，按照ascII排列('d'的ascII小)

13.线程池和进程池
# from concurrent.futures import ThreadPoolExecutor
# import time
# def func(n):
#     time.sleep(2)
#     print(n)
#
# #线程池中线程的个数一般不超过cpu的个数*5
# tpool=ThreadPoolExecutor(max_workers=5)#创建线程池
# for i in range(20):
#     tpool.submit(func,i)
# #submit()异步执行任务 五个五个的打印结果，但是不一定按0,1,2,3顺序打印，因为任务不一定是按顺序执行
# tpool.shutdown()
# print('主线程')
# #shutdown实现了停止接收任务，并在主线程执行结束之前完成线程池的任务
# # 若没有shutdown可能会先打印'主线程'，后五个五个的打印结果
# #有shutdown会先五个五个的打印结果，后打印'主线程'

#线程池 （有返回值的任务）
# from concurrent.futures import ThreadPoolExecutor
# import time
# def func(n):
#     time.sleep(2)
#     print(n) #任务不一定是按顺序执行
#     return n*n
#
# #线程池中线程的个数一般不超过cpu的个数*5
# tpool=ThreadPoolExecutor(max_workers=5)#创建线程池
# t_lst=[]
# for i in range(20):
#     t=tpool.submit(func,i)
#     t_lst.append(t)
#     #任务是按顺序加入列表中的
# tpool.shutdown()
# print('主线程')
# #t.result()获取任务的执行的结果
# for t in t_lst: print('***',t.result())
# #t.result()获取的结果是按顺序的（0，1，4，9，...），因为任务是按顺序加入列表中的
# #即任务是按顺序加入列表中的，但不一定是按顺序执行

#map方法
# from concurrent.futures import ThreadPoolExecutor
# import time
# def func(n):
#     time.sleep(2)
#     # print(n) #任务不一定是按顺序执行
#     return n*n
#
# #线程池中线程的个数一般不超过cpu的个数*5
# tpool=ThreadPoolExecutor(max_workers=5)#创建线程池
# ret=tpool.map(func,range(20)) #返回一个迭代器
# for i in ret:print(i)
# tpool.shutdown()
# print('主线程')

#callback()方法
# from concurrent.futures import ThreadPoolExecutor
# import time
# def func(n):
#     time.sleep(2)
#     print(n) #任务不一定是按顺序执行
#     return n*n
#
# def call_back(m): #m.result()获取任务执行结果
#     print('result is: %s'%m.result())
# #线程池中线程的个数一般不超过cpu的个数*5
# tpool=ThreadPoolExecutor(max_workers=5)#创建线程池
# for i in range(20):
#     t=tpool.submit(func,i).add_done_callback(call_back)
# tpool.shutdown()
# print('主线程')


#进程池，（进程池的方式与线程池相同）
# from concurrent.futures import ProcessPoolExecutor
# import time
# def func(n):
#     time.sleep(2)
#     print(n) #任务不一定是按顺序执行
#     return n*n
#
# #线程池中线程的个数一般不超过cpu的个数*5
# tpool=ProcessPoolExecutor(max_workers=5)#创建线程池
# t_lst=[]
# for i in range(20):
#     t=tpool.submit(func,i)
#     t_lst.append(t)
#     #任务是按顺序加入列表中的
# tpool.shutdown()
# print('主线程')
# #t.result()获取任务的执行的结果
# for t in t_lst: print('***',t.result())




  
  

  
  
  
